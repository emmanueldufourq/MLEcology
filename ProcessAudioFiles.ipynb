{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess audio files and corresponding .svl files to create X (spectrograms) and Y variables to train ML algorithms.\n",
    "\n",
    "\n",
    "#### Copyright Emmanuel Dufourq 2021\n",
    "\n",
    "#### African Institute for Mathematical Sciences\n",
    "\n",
    "#### Stellenbosch University\n",
    "\n",
    "#### EdgeAcoustics NPO\n",
    "\n",
    "#### edufourq@gmail.com\n",
    "\n",
    "<hr>\n",
    "\n",
    "### Folder structure required:\n",
    "\n",
    "Species Folder (root folder). Subfolders:\n",
    "\n",
    "----Annotations\n",
    "    \n",
    "----Audio\n",
    "    \n",
    "----Saved_Data\n",
    "    \n",
    "----DataFiles\n",
    "    \n",
    "Annotations folder should contain all the .svl files\n",
    "\n",
    "Audio folder should contain all the .WAV files\n",
    "\n",
    "DataFiles folder should contain two files: TrainingFiles.txt and TestingFiles.txt\n",
    "\n",
    "Saved_Data folder will be empty. This script will produce two outputs: X.pkl and Y.pkl which will be saved in this folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\n",
    "import numpy as np\n",
    "import random\n",
    "import librosa.display\n",
    "import librosa\n",
    "from xml.dom import minidom\n",
    "from scipy import signal\n",
    "from random import randint\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "from AnnotationReader import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    \n",
    "    def __init__(self, species_folder, lowpass_cutoff, \n",
    "                 downsample_rate, nyquist_rate, segment_duration, \n",
    "                 positive_class, background_class,\n",
    "                 augmentation_amount_positive_class,\n",
    "                 augmentation_amount_background_class,\n",
    "                 n_fft, hop_length, n_mels, f_min, f_max, file_type, audio_extension):\n",
    "        self.species_folder = species_folder\n",
    "        self.lowpass_cutoff = lowpass_cutoff\n",
    "        self.downsample_rate = downsample_rate\n",
    "        self.nyquist_rate = nyquist_rate\n",
    "        self.segment_duration = segment_duration\n",
    "        self.augmentation_amount_positive_class = augmentation_amount_positive_class\n",
    "        self.augmentation_amount_background_class = augmentation_amount_background_class\n",
    "        self.positive_class = positive_class\n",
    "        self.background_class = background_class\n",
    "        self.audio_path = self.species_folder + '/Audio/'\n",
    "        self.annotations_path = self.species_folder + '/Annotations/'\n",
    "        self.saved_data_path = self.species_folder + '/Saved_Data/'\n",
    "        self.training_files = self.species_folder + '/DataFiles/TrainingFiles.txt'\n",
    "        self.n_ftt = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.n_mels = n_mels\n",
    "        self.f_min = f_min\n",
    "        self.f_max = f_max\n",
    "        self.file_type = file_type\n",
    "        self.audio_extension = audio_extension\n",
    "        \n",
    "    def update_audio_path(self, audio_path):\n",
    "        self.audio_path = self.species_folder + '/Audio/' + audio_path\n",
    "        \n",
    "    def read_audio_file(self, file_name):\n",
    "        '''\n",
    "        file_name: string, name of file including extension, e.g. \"audio1.wav\"\n",
    "        \n",
    "        '''\n",
    "        # Get the path to the file\n",
    "        audio_folder = os.path.join(self.species_folder, 'Audio',file_name)\n",
    "        \n",
    "        # Read the amplitudes and sample rate\n",
    "        audio_amps, audio_sample_rate = librosa.load(audio_folder, sr=None)\n",
    "        \n",
    "        return audio_amps, audio_sample_rate\n",
    "    \n",
    "    def butter_lowpass(self, cutoff, nyq_freq, order=4):\n",
    "        normal_cutoff = float(cutoff) / nyq_freq\n",
    "        b, a = signal.butter(order, normal_cutoff, btype='lowpass')\n",
    "        return b, a\n",
    "\n",
    "    def butter_lowpass_filter(self, data, cutoff_freq, nyq_freq, order=4):\n",
    "        # Source: https://github.com/guillaume-chevalier/filtering-stft-and-laplace-transform\n",
    "        b, a = self.butter_lowpass(cutoff_freq, nyq_freq, order=order)\n",
    "        y = signal.filtfilt(b, a, data)\n",
    "        return y\n",
    "\n",
    "    def downsample_file(self, amplitudes, original_sr, new_sample_rate):\n",
    "        '''\n",
    "        Downsample an audio file to a given new sample rate.\n",
    "        amplitudes:\n",
    "        original_sr:\n",
    "        new_sample_rate:\n",
    "        \n",
    "        '''\n",
    "        return librosa.resample(amplitudes, \n",
    "                                original_sr, \n",
    "                                new_sample_rate, \n",
    "                                res_type='kaiser_fast'), new_sample_rate\n",
    "\n",
    "    def convert_single_to_image(self, audio):\n",
    "        '''\n",
    "        Convert amplitude values into a mel-spectrogram.\n",
    "        '''\n",
    "        S = librosa.feature.melspectrogram(audio, n_fft=self.n_ftt,hop_length=self.hop_length, \n",
    "                                           n_mels=self.n_mels, fmin=self.f_min, fmax=self.f_max)\n",
    "        \n",
    "        image = librosa.core.power_to_db(S)\n",
    "        image_np = np.asmatrix(image)\n",
    "        image_np_scaled_temp = (image_np - np.min(image_np))\n",
    "        image_np_scaled = image_np_scaled_temp / np.max(image_np_scaled_temp)\n",
    "        mean = image.flatten().mean()\n",
    "        std = image.flatten().std()\n",
    "        eps=1e-8\n",
    "        spec_norm = (image - mean) / (std + eps)\n",
    "        spec_min, spec_max = spec_norm.min(), spec_norm.max()\n",
    "        spec_scaled = (spec_norm - spec_min) / (spec_max - spec_min)\n",
    "        spectrogram = spec_scaled\n",
    "        \n",
    "        return spectrogram\n",
    "\n",
    "    def convert_all_to_image(self, segments):\n",
    "        '''\n",
    "        Convert a number of segments into their corresponding spectrograms.\n",
    "        '''\n",
    "        spectrograms = []\n",
    "        for segment in segments:\n",
    "            spectrograms.append(self.convert_single_to_image(segment))\n",
    "\n",
    "        return np.array(spectrograms)\n",
    "    \n",
    "    def add_extra_dim(self, spectrograms):\n",
    "        '''\n",
    "        Add an extra dimension to the data so that it matches\n",
    "        the input requirement of Tensorflow.\n",
    "        '''\n",
    "        spectrograms = np.reshape(spectrograms, \n",
    "                                  (spectrograms.shape[0],\n",
    "                                   spectrograms.shape[1],\n",
    "                                   spectrograms.shape[2],1))\n",
    "        return spectrograms\n",
    "    \n",
    "    def time_shift(self, audio, shift):\n",
    "        '''\n",
    "        Shift ampltitude values (to the right) by a random value.\n",
    "        Values are wrapped back to the left.\n",
    "        '''\n",
    "\n",
    "        augmented = np.zeros(len(audio))\n",
    "        augmented [0:shift] = audio[-shift:]\n",
    "        augmented [shift:] = audio[:-shift]\n",
    "        return augmented\n",
    "    \n",
    "    def augment_single_segment(self, segment, label, verbose):\n",
    "        '''\n",
    "        Augment a segment of amplitude values a number of times (pre-defined).\n",
    "        Augmenting is done by applying a time shift.\n",
    "        '''\n",
    "    \n",
    "        augmented_segments = []\n",
    "        augmented_labels = []\n",
    "        \n",
    "        if label in self.positive_class:\n",
    "            augmentation_amount = self.augmentation_amount_positive_class\n",
    "        else:\n",
    "            augmentation_amount = self.augmentation_amount_background_class\n",
    "\n",
    "        for i in range (0, augmentation_amount):\n",
    "\n",
    "            if verbose:\n",
    "                print ('augmentation iteration', i)\n",
    "                print ('----------------------------')\n",
    "\n",
    "            # Randomly select amount to shift by\n",
    "            random_time_point_segment = randint(1, len(segment)-1)\n",
    "\n",
    "            # Time shift\n",
    "            segment = self.time_shift(segment, random_time_point_segment)\n",
    "\n",
    "            # Append the augmented segments\n",
    "            augmented_segments.append(segment)\n",
    "            \n",
    "            # Append the label (a multiclass problem can be converted into\n",
    "            # one-hot encoded vectors at a later stage)\n",
    "            augmented_labels.append(label)\n",
    "\n",
    "        return augmented_segments, augmented_labels\n",
    "    \n",
    "    def getXY(self, audio_amplitudes, start_sec, annotation_duration_seconds, label, verbose):\n",
    "        '''\n",
    "        Extract a number of segments based on the user-annotations.\n",
    "        If possible, a number of segments are extracted provided\n",
    "        that the duration of the annotation is long enough. The segments\n",
    "        are extracted by shifting by 1 second in time to the right.\n",
    "        Each segment is then augmented a number of times based on a pre-defined\n",
    "        user value.\n",
    "        '''\n",
    "\n",
    "        if verbose == True:\n",
    "            print ('start_sec', start_sec)\n",
    "            print ('annotation_duration_seconds', annotation_duration_seconds)\n",
    "            print ('self.segment_duration ', self.segment_duration )\n",
    "            \n",
    "        X_augmented_segments = []\n",
    "        Y_augmented_labels = []\n",
    "            \n",
    "        # Calculate how many segments can be extracted based on the duration of\n",
    "        # the annotated duration. If the annotated duration is too short then\n",
    "        # simply extract one segment. If the annotated duration is long enough\n",
    "        # then multiple segments can be extracted.\n",
    "        if annotation_duration_seconds-self.segment_duration < 0:\n",
    "            segments_to_extract = 1\n",
    "        else:\n",
    "            segments_to_extract = annotation_duration_seconds-self.segment_duration+1\n",
    "            \n",
    "        if verbose:\n",
    "            print (\"segments_to_extract\", segments_to_extract)\n",
    "            \n",
    "        if label in self.background_class:\n",
    "            if segments_to_extract > 5:\n",
    "                segments_to_extract = 5\n",
    "            \n",
    "        for i in range (0, segments_to_extract):\n",
    "            if verbose:\n",
    "                print ('Semgnet {} of {}'.format(i, segments_to_extract-1))\n",
    "                print ('*******************')\n",
    "                \n",
    "            # Set the correct location to start with.\n",
    "            # The correct start is with respect to the location in time\n",
    "            # in the audio file start+i*sample_rate\n",
    "            start_data_observation = start_sec*self.downsample_rate+i*(self.downsample_rate)\n",
    "            # The end location is based off the start\n",
    "            end_data_observation = start_data_observation + (self.downsample_rate*self.segment_duration)\n",
    "            \n",
    "            # This case occurs when something is annotated towards the end of a file\n",
    "            # and can result in a segment which is too short.\n",
    "            if end_data_observation > len(audio_amplitudes):\n",
    "                continue\n",
    "\n",
    "            # Extract the segment of audio\n",
    "            X_audio = audio_amplitudes[start_data_observation:end_data_observation]\n",
    "\n",
    "            if verbose == True:\n",
    "                print ('start frame', start_data_observation)\n",
    "                print ('end frame', end_data_observation)\n",
    "            \n",
    "            # Augment the segment a number of times\n",
    "            X_augmented, Y_augmented = self.augment_single_segment(X_audio, label, verbose)\n",
    "            \n",
    "            # Extend the augmented segments and labels\n",
    "            X_augmented_segments.extend(X_augmented)\n",
    "            Y_augmented_labels.extend(Y_augmented)\n",
    "\n",
    "        return X_augmented_segments, Y_augmented_labels\n",
    "    \n",
    "    def save_data_to_pickle(self, X, Y):\n",
    "        '''\n",
    "        Save all of the spectrograms to a pickle file.\n",
    "        \n",
    "        '''\n",
    "        outfile = open(os.path.join(self.saved_data_path, 'X.pkl'),'wb')\n",
    "        pickle.dump(X, outfile, protocol=4)\n",
    "        outfile.close()\n",
    "        \n",
    "        outfile = open(os.path.join(self.saved_data_path, 'Y.pkl'),'wb')\n",
    "        pickle.dump(Y, outfile, protocol=4)\n",
    "        outfile.close()\n",
    "        \n",
    "    def load_data_from_pickle(self):\n",
    "        '''\n",
    "        Load all of the spectrograms from a pickle file\n",
    "        \n",
    "        '''\n",
    "        infile = open(os.path.join(self.saved_data_path, 'X.pkl'),'rb')\n",
    "        X = pickle.load(infile)\n",
    "        infile.close()\n",
    "        \n",
    "        infile = open(os.path.join(self.saved_data_path, 'Y.pkl'),'rb')\n",
    "        Y = pickle.load(infile)\n",
    "        infile.close()\n",
    "\n",
    "        return X, Y\n",
    "\n",
    "    def create_dataset(self, verbose):\n",
    "        '''\n",
    "        Create X and Y values which are inputs to a ML algorithm.\n",
    "        Annotated files (.svl) are read and the corresponding audio file (.wav)\n",
    "        is read. A low pass filter is applied, followed by downsampling. A \n",
    "        number of segments are extracted and augmented to create the final dataset.\n",
    "        Annotated files (.svl) are created using SonicVisualiser and it is assumed\n",
    "        that the \"boxes area\" layer was used to annotate the audio files.\n",
    "        '''\n",
    "        \n",
    "        # Keep track of how many calls were found in the annotation files\n",
    "        total_calls = 0\n",
    "\n",
    "        # Initialise lists to store the X and Y values\n",
    "        X_calls = []\n",
    "        Y_calls = []\n",
    "        \n",
    "        if verbose == True:\n",
    "            print ('Annotations path:',self.annotations_path+\"*.svl\")\n",
    "            print ('Audio path',self.audio_path+\"*.wav\")\n",
    "        \n",
    "        # Read all names of the training files\n",
    "        training_files = pd.read_csv(self.training_files, header=None)\n",
    "        \n",
    "        # Iterate over each annotation file\n",
    "        for training_file in training_files.values:\n",
    "            \n",
    "            file = training_file[0]\n",
    "            \n",
    "            if file_type == 'svl':\n",
    "                # Get the file name without paths and extensions\n",
    "                file_name_no_extension = file\n",
    "\n",
    "            if file_type == 'raven_caovitgibbons':\n",
    "                file_name_no_extension = file[file.rfind('-')+1:file.find('.')]\n",
    "                \n",
    "            print ('Processing:',file_name_no_extension)\n",
    "            \n",
    "            reader = AnnotationReader(file, self.species_folder, self.file_type, self.audio_extension)\n",
    "\n",
    "            self.update_audio_path(reader.get_audio_location())\n",
    "\n",
    "            # Check if the .wav file exists before processing\n",
    "            if self.audio_path+file_name_no_extension+self.audio_extension  in glob.glob(self.audio_path+\"*\"+self.audio_extension):\n",
    "\n",
    "                name = file[file.find('_')+1:file.find('.')]+self.audio_extension \n",
    "\n",
    "                # Read audio file\n",
    "                audio_amps, original_sample_rate = self.read_audio_file(self.audio_path+file_name_no_extension+self.audio_extension )\n",
    "\n",
    "                # Low pass filter\n",
    "                filtered = self.butter_lowpass_filter(audio_amps, self.lowpass_cutoff, self.nyquist_rate)\n",
    "\n",
    "                # Downsample\n",
    "                amplitudes, sample_rate = self.downsample_file(filtered, original_sample_rate, self.downsample_rate)\n",
    "\n",
    "                df, audio_file_name = reader.get_annotation_information()\n",
    "\n",
    "                for index, row in df.iterrows():\n",
    "\n",
    "                    start_seconds = math.floor(row['Start'])\n",
    "                    end_seconds = math.ceil(row['End'])\n",
    "                    label = row['Label']\n",
    "                    annotation_duration_seconds = end_seconds - start_seconds\n",
    "\n",
    "                    # Extract augmented audio segments and corresponding binary labels\n",
    "                    X_data, y_data = self.getXY(amplitudes, start_seconds, \n",
    "                                                annotation_duration_seconds, label, verbose)\n",
    "\n",
    "                    # Convert audio amplitudes to spectrograms\n",
    "                    X_data = self.convert_all_to_image(X_data)\n",
    "\n",
    "                    # Append the segments and labels\n",
    "                    X_calls.extend(X_data)\n",
    "                    Y_calls.extend(y_data)\n",
    "            else:\n",
    "                print ('Error, file', self.audio_path+file_name_no_extension+self.audio_extension,'not found.')\n",
    "        # Convert to numpy arrays\n",
    "        X_calls, Y_calls = np.asarray(X_calls), np.asarray(Y_calls)\n",
    "        \n",
    "        print (X_calls.shape)\n",
    "        print (Y_calls.shape)\n",
    "        \n",
    "        # Add extra dimension\n",
    "        X_calls = self.add_extra_dim(X_calls)\n",
    "\n",
    "        return X_calls, Y_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "Only change the species folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_folder = '/media/emmanueld/Data/Research/TransferLearning/PinTailedWhydah_CapeRobinChat' # Should contain /Audio and /Annotations\n",
    "lowpass_cutoff = 9000 # Cutt off for low pass filter\n",
    "downsample_rate = 18400 # Frequency to downsample to\n",
    "nyquist_rate = 9200 # Nyquist rate (half of sampling rate)\n",
    "segment_duration = 3 # how long should a segment be\n",
    "augmentation_amount_positive_class = 1 # how many times should a segment be augmented\n",
    "augmentation_amount_background_class = 1 # how many times should a segment be augmented\n",
    "positive_class = ['CRC']\n",
    "background_class = ['NOISE'] # which labels should be considered as background noise (also species not of interest)\n",
    "audio_extension = '.WAV'\n",
    "file_type = 'svl'\n",
    "n_fft = 1024 # Hann window length\n",
    "hop_length = 256 # Sepctrogram hop size\n",
    "n_mels = 128 # Spectrogram number of mells\n",
    "f_min = 1500 # Spectrogram, minimum frequency for call\n",
    "f_max = 10000 # Spectrogram, maximum frequency for call3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_pro = Preprocessing(species_folder, lowpass_cutoff, \n",
    "                downsample_rate, nyquist_rate, \n",
    "                segment_duration,\n",
    "                positive_class, background_class,\n",
    "                augmentation_amount_positive_class, \n",
    "                augmentation_amount_background_class,n_fft, \n",
    "                hop_length, n_mels, f_min, f_max, file_type, \n",
    "                audio_extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_calls, Y_calls = pre_pro.create_dataset(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_calls.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_calls.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(Y_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(Y_calls, bins=3, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View spectrograms that contain 'CRC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = np.where(Y_calls == 'CRC')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "librosa.display.specshow(X_calls[v[2],:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View spectrograms that contain 'PTW'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = np.where(Y_calls == 'PTW')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "librosa.display.specshow(X_calls[v[21],:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to pickle\n",
    "\n",
    "This will create X and Y variables which can then be read in for ML training.\n",
    "\n",
    "Output will be saved in species_folder/Saved_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_pro.save_data_to_pickle(X_calls, Y_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
